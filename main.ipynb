{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit - Credit Risk Model Stability\n",
    "\n",
    "### Цель: Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\\\">\n",
    "    <img src=\"https://i.ibb.co/jDyZDf0/credit-score-med.png\" width=\"700\" height=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Импортируем нужные библиотеки\n",
    "import pandas as pd # для работы с данными\n",
    "import numpy as np # для вычислений\n",
    "import matplotlib.pyplot as plt # для визулизаций\n",
    "import seaborn as sns # для визулизаций\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression # Линейная модель\n",
    "from sklearn import preprocessing # предобработка\n",
    "from sklearn import metrics # метрики\n",
    "from sklearn.model_selection import train_test_split # выборка\n",
    "from sklearn.ensemble import RandomForestClassifier # модель RandomForest\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold # CV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE # для дисбаланса классов\n",
    "\n",
    "import lightgbm as lgb # модель LightGBM\n",
    "\n",
    "import xgboost as xgb # модель XGBOOST\n",
    "\n",
    "# Импорт пакетов для настройки гиперпараметров\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle # Сохранить модель\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторим, как организуется процесс разработки DS-проектов согласно методологии CRISP-DM.\n",
    "\n",
    "Этапы модели CRISP-DM:\n",
    "1. Анализ требований\n",
    "2. Подгрузка данных\n",
    "3. Иследование и Подготовка данных\n",
    "4. Моделирование\n",
    "5. Оценка модели\n",
    "6. Внедрение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ требований"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нам необходимо построить модель-классификатор для вефолта кредита, то есть определить клиента который не будет платить.** Нет никаких ограничений по использовонию методов.\n",
    "\n",
    "**Цель: F1-score был больше 0,7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Знакомство с данными**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про данные можете узать по схеме - \"**schema.png**\". А так же можете посмотреть данные и другие решения проблемы в самой соревнований в [Kaggle](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/overview). Так как это займет очень много место советую вам посмотреть эту [запись](https://www.kaggle.com/code/sergiosaharovskiy/home-credit-crms-2024-eda-and-submission)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас имеется **4 типа** таблиц:\n",
    "* **Base** - Это базовая таблица с целевой переменной.\n",
    "* **depth=0** - Это статические объекты, напрямую привязанные к определенному идентификатору наблюдения. (case_id)\n",
    "* **depth=1** - С каждым идентификатором наблюдения связана историческая запись, индексируемая с помощью num_group1.\n",
    "* **depth=2** - С каждым case_id связана историческая запись, индексируемая как num_group 1, так и num_group 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подгрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала создаем **класс 'Pipeline'**, который отвечает за базовую обработку данных. Этот класс включает методы для преобразования типов данных, обработки дат и удаления ненужных столбцов на основе определённых критериев. Основная цель — подготовить данные к дальнейшему анализу или моделированию, обеспечив их консистентность и удобство работы с ними.\n",
    "\n",
    "В этом классе я фокусируюсь на базовой обработке с помощью следующих методов:\n",
    "* **Преобразование типов данных (set_table_dtypes)**: Приведение столбцов к правильным типам данных, что облегчает дальнейшую обработку и анализ.\n",
    "* **Обработка дат (handle_dates)**: Преобразование дат в числовые значения, представляющие количество дней относительно ключевой даты, и удаление ненужных столбцов.\n",
    "* **Удаление ненужных столбцов (basic_del_col)**: Удаление столбцов с высокой долей пропусков или низкой информативностью, чтобы улучшить качество и скорость моделирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df):\n",
    "        dtype_mappings = {\n",
    "            \"case_id\": \"int32\",\n",
    "            \"WEEK_NUM\": \"int32\",\n",
    "            \"num_group1\": \"int32\",\n",
    "            \"num_group2\": \"int32\",\n",
    "        }\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col in [\"date_decision\"]:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            elif col in dtype_mappings:\n",
    "                df[col] = df[col].astype(dtype_mappings[col])\n",
    "            elif col[-1] in [\"P\", \"A\"]:\n",
    "                df[col] = df[col].astype(\"float32\")\n",
    "            elif col.endswith(\"M\"):\n",
    "                df[col] = df[col].astype(\"string\")\n",
    "            elif col.endswith(\"D\"):\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "            elif col[-1] in ['T', 'L']:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(\"float32\")\n",
    "                except ValueError as e:\n",
    "                    df[col] = df[col].astype(\"string\")\n",
    "                    \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df):\n",
    "        if \"date_decision\" in df.columns:\n",
    "            date_decision = pd.to_datetime(df[\"date_decision\"])\n",
    "            for col in df.columns:\n",
    "                if col.endswith(\"D\"):\n",
    "                    df[col] = (pd.to_datetime(df[col]) - date_decision).dt.days.astype(\"float32\")\n",
    "            \n",
    "            df.drop(columns=[\"date_decision\"], inplace=True)\n",
    "        \n",
    "        df.drop(columns=[\"MONTH\"], errors=\"ignore\", inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def basic_del_col(df):\n",
    "        cols_to_drop = []\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].isna().mean()\n",
    "                if isnull > 0.8:\n",
    "                    cols_to_drop.append(col)\n",
    "                elif df[col].dtype == \"string\":\n",
    "                    freq = df[col].nunique()\n",
    "                    if 1 >= freq or freq > 200:\n",
    "                        cols_to_drop.append(col)\n",
    "        \n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = {col: 'max' for col in cols}\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "        expr_max = {col: 'max' for col in cols}\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col.endswith(\"M\")]\n",
    "        expr_max = {col: 'max' for col in cols}\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in [\"T\", \"L\"]]\n",
    "        expr_max = {col: 'max' for col in cols}\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = {col: 'max' for col in cols}\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = {}\n",
    "        exprs.update(Aggregator.num_expr(df)),\n",
    "        exprs.update(Aggregator.date_expr(df)),\n",
    "        exprs.update(Aggregator.str_expr(df)),\n",
    "        exprs.update(Aggregator.other_expr(df)),\n",
    "        exprs.update(Aggregator.count_expr(df))\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы создаем функции для чтения данных через 'path'. Это удобно, особенно когда у нас большое количество таблиц. Функции также включают базовую обработку данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем функцию для создания новых признаков и объединения нескольких DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT            = Path(\"C:\\\\Users\\\\moona\\\\Desktop\\\\HomeCredit contest\\\\\") # расположение папки\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\" # путь к данным для train\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\" # путь к данным для test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание хранилища данных\n",
    "\n",
    "Для удобства мы создаем хранилище в виде словаря. Как вы уже знаете, у нас есть четыре типа таблиц. У меня получилось загрузить большую часть таблиц но осталась одна, и из за проблемы с MemoryError я не смог загрузить ее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_in_stream(result, path, depth = 0):\n",
    "    try:        \n",
    "        df = pd.DataFrame()\n",
    "        parquet_file = pq.ParquetFile(path)\n",
    "        for batch in parquet_file.iter_batches():\n",
    "            df_batch = batch.to_pandas()\n",
    "            \n",
    "            df = pd.concat([df, df_batch], ignore_index=True)\n",
    "        \n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        \n",
    "        if depth in [1, 2]:\n",
    "            df = df.groupby(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        \n",
    "        df = Pipeline.basic_del_col(df)\n",
    "        \n",
    "        name = (((path.split(\"\\\\\")[-1]).split(\".\"))[0]).split(\"_\")[-1]\n",
    "        \n",
    "        if \"base\" in path:\n",
    "            result = df.copy()\n",
    "            result['month_decision'] = result['date_decision'].dt.month\n",
    "            result['weekday_decision'] = result['date_decision'].dt.weekday\n",
    "        else:\n",
    "            result = result.merge(df, how=\"left\", on=\"case_id\", suffixes=('', f'_{name}'))\n",
    "            \n",
    "        \n",
    "        print(f\"{path} has been successfully loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return result\n",
    "\n",
    "def load_regex_files_in_stream(result, regex_path, depth=0):\n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        # Use glob to find files matching the regex pattern\n",
    "        for path in glob(str(regex_path)):\n",
    "            parquet_file = pq.ParquetFile(path)\n",
    "            \n",
    "            # Iterate through batches of each file\n",
    "            for batch in parquet_file.iter_batches():\n",
    "                df_batch = batch.to_pandas()\n",
    "                \n",
    "                df = pd.concat([df, df_batch], ignore_index=True)\n",
    "        \n",
    "        # Set appropriate dtypes\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        \n",
    "        # Perform aggregation if required by depth\n",
    "        if depth in [1, 2]:\n",
    "            df = df.groupby(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        \n",
    "        df = Pipeline.basic_del_col(df)\n",
    "        \n",
    "        name = (((path.split(\"\\\\\")[-1]).split(\".\"))[0]).split(\"_\")[-1]\n",
    "        \n",
    "        # Merge the current result with the new dataframe\n",
    "        result = result.merge(df, how=\"left\", on=\"case_id\", suffixes=('', f'_{name}'))\n",
    "        \n",
    "        # Get the name of the last file processed for logging\n",
    "        # name = (regex_path.split(\"\\\\\")[-1]).split(\".\")[0]\n",
    "        print(f\"{regex_path} has been successfully loaded\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {regex_path}: {e}\")\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"parquet_files\\\\train\\\\\"\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\":  [ \n",
    "        path + \"train_base.parquet\"\n",
    "        ],\n",
    "    \"depth_0\": [\n",
    "        path + \"train_static_cb_0.parquet\",\n",
    "        path + \"train_static_0_*.parquet\"\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        path + \"train_applprev_1_*.parquet\",\n",
    "        path + \"train_other_1.parquet\",\n",
    "        path + \"train_person_1.parquet\",\n",
    "        path + \"train_deposit_1.parquet\",\n",
    "        path + \"train_debitcard_1.parquet\",\n",
    "        path + \"train_tax_registry_a_1.parquet\",\n",
    "        path + \"train_tax_registry_b_1.parquet\",\n",
    "        path + \"train_tax_registry_c_1.parquet\",\n",
    "        path + \"train_credit_bureau_a_1_*.parquet\",\n",
    "        path + \"train_credit_bureau_b_1.parquet\"\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        path + \"train_applprev_2.parquet\",\n",
    "        path + \"train_person_2.parquet\",\n",
    "        path + \"train_credit_bureau_b_2.parquet\",\n",
    "        # path + 'train_credit_bureau_a_2_*.parquet'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_base ['parquet_files\\\\train\\\\train_base.parquet']\n",
      "depth_0 ['parquet_files\\\\train\\\\train_static_cb_0.parquet', 'parquet_files\\\\train\\\\train_static_0_*.parquet']\n",
      "depth_1 ['parquet_files\\\\train\\\\train_applprev_1_*.parquet', 'parquet_files\\\\train\\\\train_other_1.parquet', 'parquet_files\\\\train\\\\train_person_1.parquet', 'parquet_files\\\\train\\\\train_deposit_1.parquet', 'parquet_files\\\\train\\\\train_debitcard_1.parquet', 'parquet_files\\\\train\\\\train_tax_registry_a_1.parquet', 'parquet_files\\\\train\\\\train_tax_registry_b_1.parquet', 'parquet_files\\\\train\\\\train_tax_registry_c_1.parquet', 'parquet_files\\\\train\\\\train_credit_bureau_a_1_*.parquet', 'parquet_files\\\\train\\\\train_credit_bureau_b_1.parquet']\n",
      "depth_2 ['parquet_files\\\\train\\\\train_applprev_2.parquet', 'parquet_files\\\\train\\\\train_person_2.parquet', 'parquet_files\\\\train\\\\train_credit_bureau_b_2.parquet']\n"
     ]
    }
   ],
   "source": [
    "for t, path in data_store.items():\n",
    "    print(t, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First stage start:\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_base.parquet has been successfully loaded\n",
      "First stage start:\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:07<00:07,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_static_cb_0.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:34<00:00, 17.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_static_0_*.parquet has been successfully loaded\n",
      "Second stage start:\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_applprev_1_*.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [14:34<2:11:13, 874.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_other_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [14:37<48:16, 362.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_person_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [43:53<1:56:28, 998.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_deposit_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [43:59<1:00:39, 606.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_debitcard_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [44:04<32:28, 389.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_tax_registry_a_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [44:36<17:52, 268.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_tax_registry_b_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [44:50<09:14, 184.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_tax_registry_c_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [45:26<04:35, 137.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_credit_bureau_a_1_*.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [2:50:12<40:34, 2434.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_credit_bureau_b_1.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [2:50:45<00:00, 1024.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third stage start:\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_applprev_2.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [04:10<08:21, 250.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_person_2.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [13:07<06:58, 418.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet_files\\train\\train_credit_bureau_b_2.parquet has been successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [13:16<00:00, 265.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# for t, paths in data_store.items():\n",
    "#     if t in ['df_base', 'depth_0']:\n",
    "#         print(\"First stage start:\\t\")\n",
    "#         for path in tqdm(paths):\n",
    "#             if '*' in path :\n",
    "#                 result = load_regex_files_in_stream(result, path)\n",
    "#             else:\n",
    "#                 result = load_files_in_stream(result, path)\n",
    "#     elif t in [\"depth_1\"]:\n",
    "#         print(\"Second stage start:\\t\")\n",
    "#         for path in tqdm(paths):\n",
    "#             if '*' in path :\n",
    "#                 result = load_regex_files_in_stream(result, path, 1)\n",
    "#             else:\n",
    "#                 result = load_files_in_stream(result, path, 1)\n",
    "#     else:\n",
    "#         print(\"Third stage start:\\t\")\n",
    "#         for path in tqdm(paths):\n",
    "#             if '*' in path :\n",
    "#                 result = load_regex_files_in_stream(result, path, 2)\n",
    "#             else:\n",
    "#                 result = load_files_in_stream(result, path, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"result_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moona\\AppData\\Local\\Temp\\ipykernel_11444\\509408051.py:1: DtypeWarning: Columns (8,9,15,16,19,20,24,25,67,73,75,208,227,230,233,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,323,324,325,326,327,328,329,330,331,332,333,334,354) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(\"result_train.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"result_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.pipe(Pipeline.handle_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1526659, 355)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель этого этапа — подготовить обучающую выборку для использования в моделировании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель:** Классификация\n",
    "\n",
    "**Задачи:**\n",
    "* Анализ дубликатов\n",
    "* Удаление пустот (nan > 0.75)\n",
    "* Замена пустот\n",
    "* Анализ колонок по типам данных\n",
    "* Удаление выбросов\n",
    "* EDA\n",
    "* Обработать числовые метрики\n",
    "* Обработать категориальные метрики\n",
    "* Удаление лишних метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Базовое работа с колонками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.set_index(['case_id'], inplace=True) # устанавливаем case_id как index\n",
    "df_train.drop(columns='Unnamed: 0', inplace=True) # удаляем не нужную колонку\n",
    "df_train.rename(columns=lambda x: re.sub(r'[ .?,]', '', x), inplace=True) # удаляем нечитаемые символы с колонок "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train.columns:\n",
    "    if any(char in col for char in [' ', '.', ',', '?']):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = df_train.copy() # Для удобной работы с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ дубликатов\n",
    "\n",
    "Выводим дубликаты с помощью функции duplicated() и удаляем их функцией drop_duplicates()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "      <th>month_decision</th>\n",
       "      <th>weekday_decision</th>\n",
       "      <th>birthdate_574D</th>\n",
       "      <th>dateofbirth_337D</th>\n",
       "      <th>days120_123L</th>\n",
       "      <th>days180_256L</th>\n",
       "      <th>days30_165L</th>\n",
       "      <th>...</th>\n",
       "      <th>num_group2</th>\n",
       "      <th>conts_role_79M</th>\n",
       "      <th>empls_economicalst_849M</th>\n",
       "      <th>num_group1_21</th>\n",
       "      <th>num_group2_2</th>\n",
       "      <th>pmts_dpdvalue_108P</th>\n",
       "      <th>pmts_pmtsoverdue_635A</th>\n",
       "      <th>pmts_date_1107D</th>\n",
       "      <th>num_group1_22</th>\n",
       "      <th>num_group2_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 354 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [case_id, WEEK_NUM, target, month_decision, weekday_decision, birthdate_574D, dateofbirth_337D, days120_123L, days180_256L, days30_165L, days360_512L, days90_310L, education_1103M, education_88M, firstquarter_103L, fourthquarter_440L, maritalst_385M, maritalst_893M, numberofqueries_373L, pmtscount_423L, pmtssum_45A, responsedate_1012D, responsedate_4527233D, secondquarter_766L, thirdquarter_1082L, actualdpdtolerance_344P, amtinstpaidbefduel24m_4187115A, annuity_780A, annuitynextmonth_57A, applicationcnt_361L, applications30d_658L, applicationscnt_1086L, applicationscnt_464L, applicationscnt_629L, applicationscnt_867L, avgdbddpdlast24m_3658932P, avgdbddpdlast3m_4187120P, avgdbdtollast24m_4525197P, avgdpdtolclosure24_3658938P, avginstallast24m_3658937A, avgmaxdpdlast9m_3716943P, avgoutstandbalancel6m_4187114A, avgpmtlast12m_4525200A, clientscnt12m_3712952L, clientscnt3m_3712950L, clientscnt6m_3712949L, clientscnt_100L, clientscnt_1022L, clientscnt_1071L, clientscnt_1130L, clientscnt_157L, clientscnt_257L, clientscnt_304L, clientscnt_360L, clientscnt_493L, clientscnt_533L, clientscnt_887L, clientscnt_946L, cntincpaycont9m_3716944L, cntpmts24_3658933L, commnoinclast6m_3546845L, credamount_770A, currdebt_22A, currdebtcredtyperange_828A, datefirstoffer_1144D, datelastunpaid_3546854D, daysoverduetolerancedd_3976961L, deferredmnthsnum_166L, disbursedcredamount_1113A, downpmt_116A, dtlastpmtallstes_4499206D, eir_270L, firstclxcampaign_1125D, firstdatedue_489D, homephncnt_628L, interestrate_311L, isbidproduct_1095L, lastactivateddate_801D, lastapplicationdate_877D, lastapprcommoditycat_1041M, lastapprcredamount_781A, lastapprdate_640D, lastcancelreason_561M, lastdelinqdate_224D, lastrejectcommoditycat_161M, lastrejectcredamount_222A, lastrejectdate_50D, lastrejectreason_759M, lastrejectreasonclient_4145040M, lastst_736L, maininc_215A, mastercontrelectronic_519L, mastercontrexist_109L, maxannuity_159A, maxdbddpdlast1m_3658939P, maxdbddpdtollast12m_3658940P, maxdbddpdtollast6m_4187119P, maxdebt4_972A, maxdpdfrom6mto36m_3546853P, maxdpdinstldate_3546855D, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 354 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data[temp_data.duplicated()] # ищем дубликаты в таблице temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас в данных не имеются дубликаты которые  могли бы нам помешать во время дальнейшей работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удаление пустот\n",
    "\n",
    "Выводим пустоты с колонок и удаляем колонки с пустотыми у которых пустот больше чем 75% и те колонки с типом string у которого больше 200 уникальных знечений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "residualamount_1093A       0.989438\n",
       "credlmt_228A               0.989437\n",
       "interestrateyearly_538L    0.986836\n",
       "instlamount_892A           0.983367\n",
       "numberofinstls_810L        0.983367\n",
       "                             ...   \n",
       "applicationcnt_361L        0.000000\n",
       "applications30d_658L       0.000000\n",
       "applicationscnt_1086L      0.000000\n",
       "applicationscnt_464L       0.000000\n",
       "applicationscnt_629L       0.000000\n",
       "Length: 353, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(temp_data.isna().mean()).sort_values(ascending = False) # выводим и сортируем колонок по пустотам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1526659, 292)\n"
     ]
    }
   ],
   "source": [
    "temp_data = temp_data.pipe(Pipeline.basic_del_col)\n",
    "\n",
    "print(temp_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amount_4527230A          0.700042\n",
       "num_group1_14            0.700042\n",
       "recorddate_4527225D      0.700042\n",
       "num_group1_16            0.684104\n",
       "processingdate_168D      0.684104\n",
       "                           ...   \n",
       "applicationscnt_867L     0.000000\n",
       "applications30d_658L     0.000000\n",
       "applicationscnt_1086L    0.000000\n",
       "annuity_780A             0.000000\n",
       "applicationcnt_361L      0.000000\n",
       "Length: 292, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(temp_data.isna().mean()).sort_values(ascending = False) # и еще раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Замена пустот\n",
    "\n",
    "Для хорошего прохождения следующего этапа и хорошого результата нам нужно заменить пустоты. Я буду заменять пустоты медианным значением или модой (для данных типа object) по их классам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    1478665\n",
       "1      47994\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillnans(temp_data):\n",
    "    classes = list(temp_data['target'].unique())\n",
    "    float_int_cols = temp_data.select_dtypes(['int', 'float'])\n",
    "    object_cols = temp_data.select_dtypes(['object'])\n",
    "    result_temp = pd.DataFrame()\n",
    "    del_cols = []\n",
    "    for class_name in classes:\n",
    "        temp_data_class = temp_data[temp_data['target'] == class_name]\n",
    "        \n",
    "        empty_columns = temp_data_class[float_int_cols].isna().mean()\n",
    "        empty_columns = empty_columns[empty_columns == 1].index\n",
    "        del_cols = del_cols + empty_columns\n",
    "        \n",
    "        empty_columns = temp_data_class[object_cols].isna().mean()\n",
    "        empty_columns = empty_columns[empty_columns == 1].index\n",
    "        del_cols = del_cols + empty_columns\n",
    "        \n",
    "        for col in temp_data_class.columns:\n",
    "            if col in empty_columns:\n",
    "                continue\n",
    "            elif col in float_int_cols:\n",
    "                temp_data_class[col].fillna(temp_data_class[col].median(), inplace=True)\n",
    "            else:\n",
    "                temp_data_class[col].fillna(temp_data_class[col].mode().iloc[0], inplace=True)\n",
    "        \n",
    "        result_temp = pd.concat([result_temp, temp_data_class], ignore_index=True)\n",
    "        \n",
    "        del temp_data_class\n",
    "        gc.collect()\n",
    "    \n",
    "    result_temp.drop(columns=del_cols, inplace=True)\n",
    "    result_temp = result_temp.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return result_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_train\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.3 MiB for an array with shape (1478665,) and data type uint64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m temp_data_new \u001b[38;5;241m=\u001b[39m \u001b[43mfillnans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_data_new\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mfillnans\u001b[1;34m(temp_data)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_name \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[0;32m      8\u001b[0m     temp_data_class \u001b[38;5;241m=\u001b[39m temp_data[temp_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m class_name]\n\u001b[1;32m---> 10\u001b[0m     empty_columns \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_data_class\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfloat_int_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     11\u001b[0m     empty_columns \u001b[38;5;241m=\u001b[39m empty_columns[empty_columns \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m     12\u001b[0m     del_cols \u001b[38;5;241m=\u001b[39m del_cols \u001b[38;5;241m+\u001b[39m empty_columns\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4089\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4087\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) DataFrame?\u001b[39;00m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, DataFrame):\n\u001b[1;32m-> 4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   4092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:10984\u001b[0m, in \u001b[0;36mNDFrame.where\u001b[1;34m(self, cond, other, inplace, axis, level)\u001b[0m\n\u001b[0;32m  10977\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m  10978\u001b[0m                 _chained_assignment_warning_method_msg,\n\u001b[0;32m  10979\u001b[0m                 \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m  10980\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m  10981\u001b[0m             )\n\u001b[0;32m  10983\u001b[0m other \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mapply_if_callable(other, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m> 10984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:10660\u001b[0m, in \u001b[0;36mNDFrame._where\u001b[1;34m(self, cond, other, inplace, axis, level, warn)\u001b[0m\n\u001b[0;32m  10654\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m  10655\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[0;32m  10656\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m  10657\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDowncasting object dtype arrays\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m  10658\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m  10659\u001b[0m     )\n\u001b[1;32m> 10660\u001b[0m     cond \u001b[38;5;241m=\u001b[39m \u001b[43mcond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10661\u001b[0m cond \u001b[38;5;241m=\u001b[39m cond\u001b[38;5;241m.\u001b[39minfer_objects(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m  10663\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoolean array expected for the condition, not \u001b[39m\u001b[38;5;132;01m{dtype}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:7434\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[1;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[0;32m   7432\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   7433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 7434\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\n\u001b[0;32m   7436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   7438\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotna(), value)\u001b[38;5;241m.\u001b[39m_mgr\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\base.py:186\u001b[0m, in \u001b[0;36mDataManager.fillna\u001b[1;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Do this validation even if we go through one of the no-op paths\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     limit \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mvalidate_limit(\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfillna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1699\u001b[0m, in \u001b[0;36mBlock.fillna\u001b[1;34m(self, value, limit, inplace, downcast, using_cow, already_warned)\u001b[0m\n\u001b[0;32m   1692\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(value, \u001b[38;5;241m~\u001b[39mmask\u001b[38;5;241m.\u001b[39mT, _downcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;66;03m# Note: blk._maybe_downcast vs self._maybe_downcast(nbs)\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;66;03m#  makes a difference bc blk may have object dtype, which has\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;66;03m#  different behavior in _maybe_downcast.\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extend_blocks(\n\u001b[0;32m   1698\u001b[0m     [\n\u001b[1;32m-> 1699\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_downcast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1700\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mblk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_cow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfillna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1702\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m nbs\n\u001b[0;32m   1703\u001b[0m     ]\n\u001b[0;32m   1704\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:555\u001b[0m, in \u001b[0;36mBlock._maybe_downcast\u001b[1;34m(self, blocks, downcast, using_cow, caller)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caller \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfillna\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture.no_silent_downcasting\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n\u001b[0;32m    554\u001b[0m nbs \u001b[38;5;241m=\u001b[39m extend_blocks(\n\u001b[1;32m--> 555\u001b[0m     [\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_cow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n\u001b[0;32m    556\u001b[0m )\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caller \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfillna\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nbs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    559\u001b[0m         x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nbs, blocks)\n\u001b[0;32m    560\u001b[0m     ):\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;66;03m# GH#54261\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moona\\Desktop\\HomeCredit contest\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:655\u001b[0m, in \u001b[0;36mBlock.convert\u001b[1;34m(self, copy, using_cow)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# the check above ensures we only get here with values.shape[0] == 1,\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# avoid doing .ravel as that might make a copy\u001b[39;00m\n\u001b[0;32m    653\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 655\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m res_values \u001b[38;5;129;01mis\u001b[39;00m values:\n",
      "File \u001b[1;32mlib.pyx:2541\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.3 MiB for an array with shape (1478665,) and data type uint64"
     ]
    }
   ],
   "source": [
    "temp_data_new = fillnans(temp_data)\n",
    "\n",
    "print(f\"Before: {temp_data.shape}\")\n",
    "print(f\"After: {temp_data_new.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = temp_data_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp_data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ колонок по типам данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Колонки типа float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data['num_group1_11'] = temp_data['num_group1_11'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = list((temp_data.select_dtypes('float')).columns)\n",
    "float_data = pd.DataFrame({'nunique':temp_data[float_cols].nunique(), \n",
    "              'max':temp_data[float_cols].max(), \n",
    "              'min':temp_data[float_cols].min()}, index=float_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols_1_unique = list(float_data[float_data['nunique'] == 1].index)\n",
    "for col in float_cols_1_unique:\n",
    "    temp_data[col] = temp_data[col].fillna(-1000).astype('int32') # указываю пустоты как -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols_to_int = list(float_data[abs(float_data['max'] - float_data['min']) == float_data['nunique']].index)\n",
    "for col in float_cols_to_int:\n",
    "    temp_data[col] = temp_data[col].fillna(-1000).astype('int32') # указываю пустоты как -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols_to_int_2_unique = list(float_data[float_data['nunique'] == 2].index)\n",
    "for col in float_cols_to_int_2_unique:\n",
    "    temp_data[col] = temp_data[col].fillna(-1000).astype('int32') # указываю пустоты как -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = list((temp_data.select_dtypes('float')).columns)\n",
    "float_data = pd.DataFrame({'nunique':temp_data[float_cols].nunique(), \n",
    "              'max':temp_data[float_cols].max(), \n",
    "              'min':temp_data[float_cols].min()}, index=float_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Колонки типа int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = list((temp_data.select_dtypes('int')).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Колонки типа object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = list((temp_data.select_dtypes('object')).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_data.columns) == len(float_cols) + len(int_cols) + len(obj_cols) # Проверяем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ и Удаление выбросов\n",
    "\n",
    "В этом этапе мы будем анализировать и удалять выбросы с колонок. Это поможет нам приблизиться к более хорошому результату."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
